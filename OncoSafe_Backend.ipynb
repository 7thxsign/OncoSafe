{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_BIQ0mScXn",
        "outputId": "e8228979-7d97-4845-e8af-2a026509725b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJr3b4DcSzSf",
        "outputId": "0b92aa1b-5c84-46ca-ab1b-ca17b60716a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully.\n",
            "Shape of dataset: (793, 5)\n",
            "Shape after dropping NA: (793, 5)\n",
            "Number of unique known interaction pairs: 698\n",
            "Unique risk ratings: ['C' 'D' 'X'] (3)\n",
            "Unique mechanisms: 413\n",
            "Unique alternatives: 86\n",
            "\n",
            "Training Risk Rating model...\n",
            "\n",
            "Risk Rating Model Evaluation (on known interactions test set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           D       0.97      0.88      0.92       138\n",
            "           X       0.52      0.81      0.63        21\n",
            "\n",
            "    accuracy                           0.87       159\n",
            "   macro avg       0.74      0.85      0.78       159\n",
            "weighted avg       0.91      0.87      0.89       159\n",
            "\n",
            "\n",
            "Training Mechanism of Interaction model...\n",
            "\n",
            "Mechanism Model Evaluation (on known interactions test set):\n",
            "Accuracy: 0.30\n",
            "\n",
            "Training Alternatives model...\n",
            "\n",
            "Alternatives Model Evaluation (on known interactions test set):\n",
            "Accuracy: 0.67\n",
            "\n",
            "--- Models trained. ---\n",
            "Note: Low accuracy for Mechanism/Alternatives is expected due to the high number of unique text categories.\n",
            "The 'predictive' mode for these will be an educated guess from the learned categories.\n",
            "Trained pipelines saved.\n",
            "LabelEncoders saved.\n",
            "Known interactions dictionary saved.\n",
            "All models and supporting files saved to: /content/ddi_models/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # For handling potential NaN issues if not dropped\n",
        "\n",
        "# Step 3: Load the dataset\n",
        "# Adjust the path if necessary\n",
        "try:\n",
        "    # If uploaded to Colab's root\n",
        "    df = pd.read_csv('/content/drug_interactions.csv')\n",
        "except FileNotFoundError:\n",
        "    # If in Google Drive (example path)\n",
        "    df = pd.read_csv('/content/drive/MyDrive/drug_interactions.csv') # CHANGE THIS PATH if needed\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n",
        "print(f\"Shape of dataset: {df.shape}\")\n",
        "df.head()\n",
        "\n",
        "# Step 4: Preprocessing\n",
        "# Drop rows with missing critical values, especially target variables\n",
        "df.dropna(subset=['DRUG 1', 'DRUG 2', 'MECHANISM OF INTERACTION', 'ALTERNATIVES', 'RISK RATING'], inplace=True)\n",
        "print(f\"Shape after dropping NA: {df.shape}\")\n",
        "\n",
        "# Standardize drug names (convert to string, uppercase, strip whitespace)\n",
        "# This helps in consistent lookup and feature creation\n",
        "df['DRUG 1'] = df['DRUG 1'].astype(str).str.upper().str.strip()\n",
        "df['DRUG 2'] = df['DRUG 2'].astype(str).str.upper().str.strip()\n",
        "\n",
        "# Create a canonical representation for drug pairs to handle (Drug A, Drug B) == (Drug B, Drug A)\n",
        "# This will be used for creating a lookup dictionary of known interactions\n",
        "df['PAIR_KEY'] = df.apply(lambda row: tuple(sorted((row['DRUG 1'], row['DRUG 2']))), axis=1)\n",
        "\n",
        "# Store known interactions in a dictionary for quick lookup\n",
        "known_interactions = {}\n",
        "for _, row in df.iterrows():\n",
        "    known_interactions[row['PAIR_KEY']] = {\n",
        "        'mechanism': row['MECHANISM OF INTERACTION'],\n",
        "        'alternatives': row['ALTERNATIVES'],\n",
        "        'risk': row['RISK RATING']\n",
        "    }\n",
        "print(f\"Number of unique known interaction pairs: {len(known_interactions)}\")\n",
        "\n",
        "# Prepare features for the model: combine DRUG 1 and DRUG 2 into a single text string\n",
        "# Sorting them ensures that \"DrugA DrugB\" is treated the same as \"DrugB DrugA\" by the vectorizer\n",
        "df['INPUT_FEATURES'] = df.apply(lambda row: ' '.join(sorted((row['DRUG 1'], row['DRUG 2']))), axis=1)\n",
        "\n",
        "# Prepare target variables\n",
        "y_risk = df['RISK RATING']\n",
        "y_mechanism = df['MECHANISM OF INTERACTION']\n",
        "y_alternatives = df['ALTERNATIVES']\n",
        "\n",
        "# Encode target variables (since classifiers need numerical targets)\n",
        "le_risk = LabelEncoder()\n",
        "le_mechanism = LabelEncoder()\n",
        "le_alternatives = LabelEncoder()\n",
        "\n",
        "y_risk_encoded = le_risk.fit_transform(y_risk)\n",
        "y_mechanism_encoded = le_mechanism.fit_transform(y_mechanism)\n",
        "y_alternatives_encoded = le_alternatives.fit_transform(y_alternatives)\n",
        "\n",
        "print(f\"Unique risk ratings: {le_risk.classes_} ({len(le_risk.classes_)})\")\n",
        "print(f\"Unique mechanisms: {len(le_mechanism.classes_)}\") # Too many to print all\n",
        "print(f\"Unique alternatives: {len(le_alternatives.classes_)}\") # Too many to print all\n",
        "\n",
        "\n",
        "# Step 5: Train the models\n",
        "# We will train three separate models.\n",
        "# Using a Pipeline to combine TF-IDF vectorization and RandomForestClassifier.\n",
        "\n",
        "# Features\n",
        "X = df['INPUT_FEATURES']\n",
        "\n",
        "# Split data (optional for final model, but good for seeing some performance metrics)\n",
        "# For the final \"production\" model, you might train on ALL available data.\n",
        "# Here, we'll split to show how to evaluate.\n",
        "X_train, X_test, y_risk_train, y_risk_test, y_mech_train, y_mech_test, y_alt_train, y_alt_test = train_test_split(\n",
        "    X, y_risk_encoded, y_mechanism_encoded, y_alternatives_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- Model for RISK RATING ---\n",
        "pipeline_risk = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=2)), # ngram_range and min_df can be tuned\n",
        "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')) # class_weight='balanced' can help with imbalanced classes\n",
        "])\n",
        "print(\"\\nTraining Risk Rating model...\")\n",
        "pipeline_risk.fit(X_train, y_risk_train)\n",
        "# Evaluation (on the test set of known interactions)\n",
        "y_risk_pred_test = pipeline_risk.predict(X_test)\n",
        "print(\"\\nRisk Rating Model Evaluation (on known interactions test set):\")\n",
        "# Handle cases where some classes might not be present in predictions for the report\n",
        "# Get all unique labels from both true and predicted\n",
        "risk_labels_for_report = np.unique(np.concatenate((y_risk_test, y_risk_pred_test)))\n",
        "print(classification_report(y_risk_test, y_risk_pred_test, labels=risk_labels_for_report, target_names=le_risk.inverse_transform(risk_labels_for_report), zero_division=0))\n",
        "\n",
        "\n",
        "# --- Model for MECHANISM OF INTERACTION ---\n",
        "pipeline_mechanism = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=2)),\n",
        "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
        "])\n",
        "print(\"\\nTraining Mechanism of Interaction model...\")\n",
        "pipeline_mechanism.fit(X_train, y_mech_train)\n",
        "y_mech_pred_test = pipeline_mechanism.predict(X_test)\n",
        "print(\"\\nMechanism Model Evaluation (on known interactions test set):\")\n",
        "# This will have many classes, so accuracy might be low. The report will be very long.\n",
        "# We'll just print overall accuracy.\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"Accuracy: {accuracy_score(y_mech_test, y_mech_pred_test):.2f}\")\n",
        "# mech_labels_for_report = np.unique(np.concatenate((y_mech_test, y_mech_pred_test)))\n",
        "# print(classification_report(y_mech_test, y_mech_pred_test, labels=mech_labels_for_report, target_names=le_mechanism.inverse_transform(mech_labels_for_report), zero_division=0))\n",
        "\n",
        "\n",
        "# --- Model for ALTERNATIVES ---\n",
        "pipeline_alternatives = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=2)),\n",
        "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
        "])\n",
        "print(\"\\nTraining Alternatives model...\")\n",
        "pipeline_alternatives.fit(X_train, y_alt_train)\n",
        "y_alt_pred_test = pipeline_alternatives.predict(X_test)\n",
        "print(\"\\nAlternatives Model Evaluation (on known interactions test set):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_alt_test, y_alt_pred_test):.2f}\")\n",
        "# alt_labels_for_report = np.unique(np.concatenate((y_alt_test, y_alt_pred_test)))\n",
        "# print(classification_report(y_alt_test, y_alt_pred_test, labels=alt_labels_for_report, target_names=le_alternatives.inverse_transform(alt_labels_for_report), zero_division=0))\n",
        "\n",
        "# ... (all your existing code up to model training) ...\n",
        "\n",
        "print(\"\\n--- Models trained. ---\")\n",
        "print(\"Note: Low accuracy for Mechanism/Alternatives is expected due to the high number of unique text categories.\")\n",
        "print(\"The 'predictive' mode for these will be an educated guess from the learned categories.\")\n",
        "\n",
        "# Step 5.1: Save the trained models and LabelEncoders\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create a directory to save the models if it doesn't exist\n",
        "model_save_path = '/content/ddi_models/' # Or /content/drive/MyDrive/ddi_models/ to save to Drive\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Save the pipelines (which include TF-IDF vectorizer and classifier)\n",
        "joblib.dump(pipeline_risk, os.path.join(model_save_path, 'pipeline_risk.joblib'))\n",
        "joblib.dump(pipeline_mechanism, os.path.join(model_save_path, 'pipeline_mechanism.joblib'))\n",
        "joblib.dump(pipeline_alternatives, os.path.join(model_save_path, 'pipeline_alternatives.joblib'))\n",
        "print(\"Trained pipelines saved.\")\n",
        "\n",
        "# Save the LabelEncoders\n",
        "joblib.dump(le_risk, os.path.join(model_save_path, 'le_risk.joblib'))\n",
        "joblib.dump(le_mechanism, os.path.join(model_save_path, 'le_mechanism.joblib'))\n",
        "joblib.dump(le_alternatives, os.path.join(model_save_path, 'le_alternatives.joblib'))\n",
        "print(\"LabelEncoders saved.\")\n",
        "\n",
        "# Save the known_interactions dictionary (useful for the Flask app)\n",
        "joblib.dump(known_interactions, os.path.join(model_save_path, 'known_interactions.joblib'))\n",
        "print(\"Known interactions dictionary saved.\")\n",
        "\n",
        "print(f\"All models and supporting files saved to: {model_save_path}\")\n",
        "\n",
        "# You can then download this 'ddi_models' folder from Colab's file browser\n",
        "# or directly access it if you saved it to your Google Drive.\n",
        "\n",
        "# ... (rest of your example usage code, if you want to keep it) ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkgAeA9OXKLx",
        "outputId": "6a7172ae-cfe1-47e3-defd-09b4012595b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.7-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.7-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask\n",
        "!pip install joblib\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPOYaJkYXXfz",
        "outputId": "3f2bdc0e-9288-4a0b-9496-cd280a9f78bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import os\n",
        "import pandas as pd # Though not directly used for prediction, good to have for consistency if needed\n",
        "from pyngrok import ngrok, conf # Import pyngrok\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to the directory where you saved your models and encoders\n",
        "MODEL_DIR = 'ddi_models/' # Assume ddi_models folder is in the same directory as app.py\n",
        "                        # If not, provide the full path.\n",
        "\n",
        "NGROK_AUTHTOKEN = \" \" # YOUR NGROK AUTHTOKEN\n",
        "\n",
        "# --- Load Models and Encoders ONCE at startup ---\n",
        "pipeline_risk, pipeline_mechanism, pipeline_alternatives = None, None, None\n",
        "le_risk, le_mechanism, le_alternatives = None, None, None\n",
        "known_interactions = {}\n",
        "\n",
        "try:\n",
        "    pipeline_risk = joblib.load(os.path.join(MODEL_DIR, 'pipeline_risk.joblib'))\n",
        "    pipeline_mechanism = joblib.load(os.path.join(MODEL_DIR, 'pipeline_mechanism.joblib'))\n",
        "    pipeline_alternatives = joblib.load(os.path.join(MODEL_DIR, 'pipeline_alternatives.joblib'))\n",
        "\n",
        "    le_risk = joblib.load(os.path.join(MODEL_DIR, 'le_risk.joblib'))\n",
        "    le_mechanism = joblib.load(os.path.join(MODEL_DIR, 'le_mechanism.joblib'))\n",
        "    le_alternatives = joblib.load(os.path.join(MODEL_DIR, 'le_alternatives.joblib'))\n",
        "\n",
        "    known_interactions = joblib.load(os.path.join(MODEL_DIR, 'known_interactions.joblib'))\n",
        "    print(\"Models and encoders loaded successfully.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading model files: {e}\")\n",
        "    print(f\"Ensure the '{MODEL_DIR}' directory exists and contains all .joblib files.\")\n",
        "    print(\"The application might not function correctly without these files.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during model loading: {e}\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "def get_interaction_data(drug1, drug2):\n",
        "    \"\"\"\n",
        "    Core logic to get interaction data, similar to the Colab function.\n",
        "    \"\"\"\n",
        "    if not all([pipeline_risk, pipeline_mechanism, pipeline_alternatives, le_risk, le_mechanism, le_alternatives]):\n",
        "        return {\"error\": \"Models not loaded. Backend issue.\"}\n",
        "\n",
        "    # Standardize input drug names\n",
        "    d1_processed = str(drug1).upper().strip()\n",
        "    d2_processed = str(drug2).upper().strip()\n",
        "\n",
        "    pair_key = tuple(sorted((d1_processed, d2_processed)))\n",
        "\n",
        "    if pair_key in known_interactions:\n",
        "        interaction_data = known_interactions[pair_key]\n",
        "        return {\n",
        "            'DRUG 1': d1_processed,\n",
        "            'DRUG 2': d2_processed,\n",
        "            'MECHANISM OF INTERACTION': interaction_data['mechanism'],\n",
        "            'ALTERNATIVES': interaction_data['alternatives'],\n",
        "            'RISK RATING': interaction_data['risk'],\n",
        "            'SOURCE': 'Known from dataset'\n",
        "        }\n",
        "    else:\n",
        "        # Predictive mode\n",
        "        input_feature_string = ' '.join(sorted((d1_processed, d2_processed)))\n",
        "\n",
        "        risk_pred_encoded = pipeline_risk.predict([input_feature_string])[0]\n",
        "        mech_pred_encoded = pipeline_mechanism.predict([input_feature_string])[0]\n",
        "        alt_pred_encoded = pipeline_alternatives.predict([input_feature_string])[0]\n",
        "\n",
        "        risk_pred = le_risk.inverse_transform([risk_pred_encoded])[0]\n",
        "        mech_pred = le_mechanism.inverse_transform([mech_pred_encoded])[0]\n",
        "        alt_pred = le_alternatives.inverse_transform([alt_pred_encoded])[0]\n",
        "\n",
        "        return {\n",
        "            'DRUG 1': d1_processed,\n",
        "            'DRUG 2': d2_processed,\n",
        "            'MECHANISM OF INTERACTION': mech_pred,\n",
        "            'ALTERNATIVES': alt_pred,\n",
        "            'RISK RATING': risk_pred,\n",
        "            'SOURCE': 'Predictive (based on model learning)'\n",
        "        }\n",
        "\n",
        "@app.route('/predict_ddi', methods=['POST'])\n",
        "def predict_ddi_endpoint():\n",
        "    if not request.is_json:\n",
        "        return jsonify({\"error\": \"Request must be JSON\"}), 400\n",
        "\n",
        "    data = request.get_json()\n",
        "    drug1 = data.get('drug1')\n",
        "    drug2 = data.get('drug2')\n",
        "\n",
        "    if not drug1 or not drug2:\n",
        "        return jsonify({\"error\": \"Missing 'drug1' or 'drug2' in request body\"}), 400\n",
        "\n",
        "    try:\n",
        "        result = get_interaction_data(drug1, drug2)\n",
        "        return jsonify(result)\n",
        "    except Exception as e:\n",
        "        # Log the exception for debugging\n",
        "        app.logger.error(f\"Error during prediction: {e}\", exc_info=True)\n",
        "        return jsonify({\"error\": \"An internal server error occurred during prediction.\"}), 500\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return \"Drug Interaction Prediction API is running!\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Pyngrok Configuration ---\n",
        "    if not NGROK_AUTHTOKEN or \"YOUR_NGROK_AUTHTOKEN\" in NGROK_AUTHTOKEN : # Basic check\n",
        "        print(\"ERROR: NGROK_AUTHTOKEN is not set correctly in app.py\")\n",
        "        print(\"The application will run locally only.\")\n",
        "        # Optionally, you could exit here or just run locally without ngrok\n",
        "        # exit()\n",
        "    else:\n",
        "        try:\n",
        "            conf.get_default().auth_token = NGROK_AUTHTOKEN\n",
        "            public_url = ngrok.connect(5000) # Flask default port is 5000\n",
        "            print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:5000\\\"\")\n",
        "            print(f\" * Access your API POST endpoint at: {public_url}/predict_ddi\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not start ngrok tunnel. Error: {e}\")\n",
        "            print(\"The application will run locally only.\")\n",
        "\n",
        "    # Run Flask app\n",
        "    # Use '0.0.0.0' to make it accessible from your network, not just localhost\n",
        "    # 'debug=True' is good for development, but turn off for production\n",
        "    app.run(debug=True, host='0.0.0.0', port=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8WRzPR2YePQ",
        "outputId": "da707722-8560-4ed6-94b5-13c9bbca993e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models and encoders loaded successfully.\n",
            " * ngrok tunnel \"NgrokTunnel: \"https://a9f5-35-230-171-11.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000\"\n",
            " * Access your API POST endpoint at: NgrokTunnel: \"https://a9f5-35-230-171-11.ngrok-free.app\" -> \"http://localhost:5000\"/predict_ddi\n",
            " * Serving Flask app 'app'\n",
            " * Debug mode: on\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            " * Restarting with stat\n",
            "Models and encoders loaded successfully.\n",
            "t=2025-05-08T14:52:18+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n",
            "t=2025-05-08T14:52:18+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "t=2025-05-08T14:52:18+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "t=2025-05-08T14:52:18+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "t=2025-05-08T14:52:18+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:  authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\n",
            "ERROR:  You can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\n",
            "ERROR:  Read more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\n",
            "ERROR:  You can view your current agent sessions in the dashboard:\n",
            "ERROR:  https://dashboard.ngrok.com/agents\n",
            "ERROR:  \n",
            "ERROR:  ERR_NGROK_108\n",
            "ERROR:  https://ngrok.com/docs/errors/err_ngrok_108\n",
            "ERROR:  \n",
            "Could not start ngrok tunnel. Error: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.\n",
            "The application will run locally only.\n",
            " * Debugger is active!\n",
            " * Debugger PIN: 105-681-810\n",
            "127.0.0.1 - - [08/May/2025 14:53:18] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 14:53:32] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 14:55:06] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 14:56:10] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:21:27] \"\u001b[31m\u001b[1mGET /predict_ddi HTTP/1.1\u001b[0m\" 405 -\n",
            "127.0.0.1 - - [08/May/2025 15:21:28] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [08/May/2025 15:28:38] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:28:43] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:28:48] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:28:50] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:30:48] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:37:49] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:38:03] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:38:09] \"\u001b[31m\u001b[1mPOST /predict_ddi HTTP/1.1\u001b[0m\" 400 -\n",
            "127.0.0.1 - - [08/May/2025 15:38:22] \"\u001b[31m\u001b[1mPOST /predict_ddi HTTP/1.1\u001b[0m\" 400 -\n",
            "127.0.0.1 - - [08/May/2025 15:38:36] \"\u001b[31m\u001b[1mPOST /predict_ddi HTTP/1.1\u001b[0m\" 400 -\n",
            "127.0.0.1 - - [08/May/2025 15:39:04] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:39:08] \"\u001b[31m\u001b[1mPOST /predict_ddi HTTP/1.1\u001b[0m\" 400 -\n",
            "127.0.0.1 - - [08/May/2025 15:41:04] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:41:11] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:41:55] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:42:04] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:42:10] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:42:20] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 15:44:21] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:09:05] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:12:50] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:12:58] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:13:14] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:13:20] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:13:25] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:14:27] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:18:53] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:26:33] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:26:53] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:44:28] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:44:37] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:44:43] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:44:56] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:45:04] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:45:17] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:45:28] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:45:39] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 16:59:32] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 17:23:27] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 17:23:47] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 17:24:18] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 17:24:33] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 17:25:08] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 17:25:34] \"POST /predict_ddi HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [08/May/2025 17:25:45] \"POST /predict_ddi HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "!python app.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
